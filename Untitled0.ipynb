{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maysq/GNN4NLP-Papers/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoaqsIXIJ1fr",
        "outputId": "a8dc4474-089d-4672-dde5-688840b85d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 69 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=b2e2e9d3a08b175dd4a077fee1e61b52f66e648cb37ad12a4021f2fb5fcd8bd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install folium==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCPiYNq3Jcje",
        "outputId": "c3f25da1-ec19-4196-801a-47a420913eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.22.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.26.0,>=1.25.4\n",
            "  Downloading botocore-1.25.4-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 75.2 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 82.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.4->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.4->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 76.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.22.4 botocore-1.25.4 jmespath-1.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tECHq7jeqtkI",
        "outputId": "df133f9d-d02d-4d8f-b535-8f73792dcd75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4wvLuCvKDF0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\n",
        "from torch.utils import data\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "\n",
        "def clean_tweet_tokenize(string):\n",
        "    tknzr = TweetTokenizer(\n",
        "        reduce_len=True, preserve_case=False, strip_handles=False)\n",
        "    tokens = tknzr.tokenize(string.lower())\n",
        "    return ' '.join(tokens).strip()\n",
        "\n",
        "def normalize_adj(adj):\n",
        "    \n",
        "    # adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1)) #D-degree matrix\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
        "\n",
        "def sparse_scipy2torch(coo_sparse):\n",
        "    # coo_sparse=coo_sparse.tocoo()\n",
        "    i=torch.LongTensor(np.vstack((coo_sparse.row, coo_sparse.col)))\n",
        "    v=torch.from_numpy(coo_sparse.data)\n",
        "    return torch.sparse.FloatTensor(i, v, torch.Size(coo_sparse.shape))\n",
        "\n",
        "\n",
        "def get_class_count_and_weight(y,n_classes):\n",
        "    classes_count=[]\n",
        "    weight=[]\n",
        "    for i in range(n_classes):\n",
        "        count=np.sum(y==i)\n",
        "        classes_count.append(count)\n",
        "        weight.append(len(y)/(n_classes*count))\n",
        "    return classes_count,weight\n",
        "\n",
        "class InputExample(object):\n",
        "   \n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, confidence=None, label=None):\n",
        "      \n",
        "        self.guid = guid\n",
        "        # string of the sentence,example: [EU, rejects, German, call, to, boycott, British, lamb .]\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        # the label(class) for the sentence\n",
        "        self.confidence=confidence\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "   \n",
        "    def __init__(self, guid, tokens, input_ids, gcn_vocab_ids, input_mask, segment_ids, confidence, label_id):\n",
        "        self.guid = guid\n",
        "        self.tokens = tokens\n",
        "        self.input_ids = input_ids\n",
        "        self.gcn_vocab_ids=gcn_vocab_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.confidence=confidence\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "   \n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "def example2feature(example, tokenizer, gcn_vocab_map, max_seq_len, gcn_embedding_dim):\n",
        "\n",
        "    # tokens_a = tokenizer.tokenize(example.text_a)\n",
        "    # do not need use bert.tokenizer again, because be used at prepare_data.py\n",
        "    tokens_a = example.text_a.split()\n",
        "    assert example.text_b==None\n",
        "    # Account for [CLS] and [SEP] with \"- 2\" ,# -1 for gcn_words_convoled\n",
        "    if len(tokens_a) > max_seq_len - 1 - gcn_embedding_dim: \n",
        "        print('GUID: %d, Sentence is too long: %d'%(example.guid, len(tokens_a)))\n",
        "        tokens_a = tokens_a[:(max_seq_len - 1 - gcn_embedding_dim)]\n",
        "\n",
        "    gcn_vocab_ids=[]\n",
        "    for w in tokens_a:\n",
        "        gcn_vocab_ids.append(gcn_vocab_map[w])\n",
        "\n",
        "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\" for i in range(gcn_embedding_dim+1)]\n",
        "    segment_ids = [0] * len(tokens)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    feat=InputFeatures(\n",
        "            guid=example.guid,\n",
        "            tokens=tokens,\n",
        "            input_ids=input_ids,\n",
        "            gcn_vocab_ids=gcn_vocab_ids,\n",
        "            input_mask=input_mask,\n",
        "            segment_ids=segment_ids,\n",
        "            # label_id=label2idx[example.label]\n",
        "            confidence=example.confidence,\n",
        "            label_id=example.label\n",
        "    )\n",
        "    return feat\n",
        "\n",
        "class CorpusDataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer, gcn_vocab_map, max_seq_len, gcn_embedding_dim):\n",
        "        self.examples=examples\n",
        "        self.tokenizer=tokenizer\n",
        "        self.max_seq_len=max_seq_len\n",
        "        self.gcn_embedding_dim=gcn_embedding_dim\n",
        "        self.gcn_vocab_map=gcn_vocab_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feat=example2feature(self.examples[idx], self.tokenizer, self.gcn_vocab_map, self.max_seq_len, self.gcn_embedding_dim)\n",
        "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.confidence, feat.label_id, feat.gcn_vocab_ids\n",
        "\n",
        "    \n",
        "    def pad(self,batch):\n",
        "        gcn_vocab_size=len(self.gcn_vocab_map)\n",
        "        seqlen_list = [len(sample[0]) for sample in batch]\n",
        "        maxlen = np.array(seqlen_list).max()\n",
        "\n",
        "        f_collect = lambda x: [sample[x] for sample in batch]\n",
        "        f_pad = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch]\n",
        "        # filliing with -1, for indicate the position of this pad is not in gcn_vocab_list. then for generate the transform order tensor and delete this column.\n",
        "        # first -1 correspond[CLS]\n",
        "        f_pad2 = lambda x, seqlen: [[-1]+ sample[x] + [-1] * (seqlen - len(sample[x])-1) for sample in batch]\n",
        "\n",
        "        batch_input_ids = torch.tensor(f_pad(0, maxlen), dtype=torch.long)\n",
        "        batch_input_mask = torch.tensor(f_pad(1, maxlen), dtype=torch.long)\n",
        "        batch_segment_ids = torch.tensor(f_pad(2, maxlen), dtype=torch.long)\n",
        "        batch_confidences = torch.tensor(f_collect(3), dtype=torch.float)\n",
        "        batch_label_ids = torch.tensor(f_collect(4), dtype=torch.long)\n",
        "        batch_gcn_vocab_ids_paded = np.array(f_pad2(5, maxlen)).reshape(-1)\n",
        "        #generate eye matrix according to gcn_vocab_size+1, the 1 is for f_pad2 filling -1, then change to the row with all 0 value.\n",
        "        batch_gcn_swop_eye=torch.eye(gcn_vocab_size+1)[batch_gcn_vocab_ids_paded][:,:-1]\n",
        "        #This tensor is for transform batch_embedding_tensor to gcn_vocab order\n",
        "        # -1 is seq_len. usage: batch_gcn_swop_eye.matmul(batch_seq_embedding)\n",
        "        batch_gcn_swop_eye=batch_gcn_swop_eye.view(len(batch),-1,gcn_vocab_size).transpose(1,2)\n",
        "\n",
        "        return batch_input_ids, batch_input_mask, batch_segment_ids, batch_confidences, batch_label_ids, batch_gcn_swop_eye\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IffzdZ_rSVZi"
      },
      "outputs": [],
      "source": [
        "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import fnmatch\n",
        "from functools import wraps\n",
        "from hashlib import sha256\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "#import boto3\n",
        "import requests\n",
        "#from botocore.exceptions import ClientError\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from torch.hub import _get_torch_home\n",
        "    torch_cache_home = _get_torch_home()\n",
        "except ImportError:\n",
        "    torch_cache_home = os.path.expanduser(\n",
        "        os.getenv('TORCH_HOME', os.path.join(\n",
        "            os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\n",
        "default_cache_path = os.path.join(torch_cache_home, 'pytorch_pretrained_bert')\n",
        "\n",
        "try:\n",
        "    from urllib.parse import urlparse\n",
        "except ImportError:\n",
        "    from urlparse import urlparse\n",
        "\n",
        "try:\n",
        "    from pathlib import Path\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n",
        "        os.getenv('PYTORCH_PRETRAINED_BERT_CACHE', default_cache_path))\n",
        "except (AttributeError, ImportError):\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
        "                                              default_cache_path)\n",
        "\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
        "\n",
        "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
        "\n",
        "\n",
        "def url_to_filename(url, etag=None):\n",
        "    \"\"\"\n",
        "    Convert `url` into a hashed filename in a repeatable way.\n",
        "    If `etag` is specified, append its hash to the url's, delimited\n",
        "    by a period.\n",
        "    \"\"\"\n",
        "    url_bytes = url.encode('utf-8')\n",
        "    url_hash = sha256(url_bytes)\n",
        "    filename = url_hash.hexdigest()\n",
        "\n",
        "    if etag:\n",
        "        etag_bytes = etag.encode('utf-8')\n",
        "        etag_hash = sha256(etag_bytes)\n",
        "        filename += '.' + etag_hash.hexdigest()\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def filename_to_url(filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Return the url and etag (which may be ``None``) stored for `filename`.\n",
        "    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "    if not os.path.exists(cache_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(cache_path))\n",
        "\n",
        "    meta_path = cache_path + '.json'\n",
        "    if not os.path.exists(meta_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(meta_path))\n",
        "\n",
        "    with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
        "        metadata = json.load(meta_file)\n",
        "    url = metadata['url']\n",
        "    etag = metadata['etag']\n",
        "\n",
        "    return url, etag\n",
        "\n",
        "\n",
        "def cached_path(url_or_filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given something that might be a URL (or might be a local path),\n",
        "    determine which. If it's a URL, download the file and cache it, and\n",
        "    return the path to the cached file. If it's already a local path,\n",
        "    make sure the file exists and then return the path.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n",
        "        url_or_filename = str(url_or_filename)\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    parsed = urlparse(url_or_filename)\n",
        "\n",
        "    if parsed.scheme in ('http', 'https', 's3'):\n",
        "        # URL, so get it from the cache (downloading if necessary)\n",
        "        return get_from_cache(url_or_filename, cache_dir)\n",
        "    elif os.path.exists(url_or_filename):\n",
        "        # File, and it exists.\n",
        "        return url_or_filename\n",
        "    elif parsed.scheme == '':\n",
        "        # File, but it doesn't exist.\n",
        "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
        "    else:\n",
        "        # Something unknown\n",
        "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
        "\n",
        "\n",
        "def split_s3_path(url):\n",
        "    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.netloc or not parsed.path:\n",
        "        raise ValueError(\"bad s3 path {}\".format(url))\n",
        "    bucket_name = parsed.netloc\n",
        "    s3_path = parsed.path\n",
        "    # Remove '/' at beginning of path.\n",
        "    if s3_path.startswith(\"/\"):\n",
        "        s3_path = s3_path[1:]\n",
        "    return bucket_name, s3_path\n",
        "\n",
        "\n",
        "def s3_request(func):\n",
        "    \"\"\"\n",
        "    Wrapper function for s3 requests in order to create more helpful error\n",
        "    messages.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(url, *args, **kwargs):\n",
        "        try:\n",
        "            return func(url, *args, **kwargs)\n",
        "        except ClientError as exc:\n",
        "            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n",
        "                raise EnvironmentError(\"file {} not found\".format(url))\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_etag(url):\n",
        "    \"\"\"Check ETag on S3 object.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_object = s3_resource.Object(bucket_name, s3_path)\n",
        "    return s3_object.e_tag\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_get(url, temp_file):\n",
        "    \"\"\"Pull a file directly from S3.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n",
        "\n",
        "\n",
        "def http_get(url, temp_file):\n",
        "    req = requests.get(url, stream=True)\n",
        "    content_length = req.headers.get('Content-Length')\n",
        "    total = int(content_length) if content_length is not None else None\n",
        "    progress = tqdm(unit=\"B\", total=total)\n",
        "    for chunk in req.iter_content(chunk_size=1024):\n",
        "        if chunk: # filter out keep-alive new chunks\n",
        "            progress.update(len(chunk))\n",
        "            temp_file.write(chunk)\n",
        "    progress.close()\n",
        "\n",
        "\n",
        "def get_from_cache(url, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given a URL, look for the corresponding dataset in the local cache.\n",
        "    If it's not there, download it. Then return the path to the cached file.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    # Get eTag to add to filename, if it exists.\n",
        "    if url.startswith(\"s3://\"):\n",
        "        etag = s3_etag(url)\n",
        "    else:\n",
        "        try:\n",
        "            response = requests.head(url, allow_redirects=True)\n",
        "            if response.status_code != 200:\n",
        "                etag = None\n",
        "            else:\n",
        "                etag = response.headers.get(\"ETag\")\n",
        "        except EnvironmentError:\n",
        "            etag = None\n",
        "\n",
        "    if sys.version_info[0] == 2 and etag is not None:\n",
        "        etag = etag.decode('utf-8')\n",
        "    filename = url_to_filename(url, etag)\n",
        "\n",
        "    # get cache path to put the file\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "\n",
        "    # If we don't have a connection (etag is None) and can't identify the file\n",
        "    # try to get the last downloaded one\n",
        "    if not os.path.exists(cache_path) and etag is None:\n",
        "        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + '.*')\n",
        "        matching_files = list(filter(lambda s: not s.endswith('.json'), matching_files))\n",
        "        if matching_files:\n",
        "            cache_path = os.path.join(cache_dir, matching_files[-1])\n",
        "\n",
        "    if not os.path.exists(cache_path):\n",
        "        # Download to temporary file, then copy to cache dir once finished.\n",
        "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
        "        with tempfile.NamedTemporaryFile() as temp_file:\n",
        "            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n",
        "\n",
        "            # GET file object\n",
        "            if url.startswith(\"s3://\"):\n",
        "                s3_get(url, temp_file)\n",
        "            else:\n",
        "                http_get(url, temp_file)\n",
        "\n",
        "            # we are copying the file before closing it, so flush to avoid truncation\n",
        "            temp_file.flush()\n",
        "            # shutil.copyfileobj() starts at the current position, so go to the start\n",
        "            temp_file.seek(0)\n",
        "\n",
        "            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n",
        "            with open(cache_path, 'wb') as cache_file:\n",
        "                shutil.copyfileobj(temp_file, cache_file)\n",
        "\n",
        "            logger.info(\"creating metadata file for %s\", cache_path)\n",
        "            meta = {'url': url, 'etag': etag}\n",
        "            meta_path = cache_path + '.json'\n",
        "            with open(meta_path, 'w') as meta_file:\n",
        "                output_string = json.dumps(meta)\n",
        "                if sys.version_info[0] == 2 and isinstance(output_string, str):\n",
        "                    output_string = unicode(output_string, 'utf-8')  # The beauty of python 2\n",
        "                meta_file.write(output_string)\n",
        "\n",
        "            logger.info(\"removing temp file %s\", temp_file.name)\n",
        "\n",
        "    return cache_path\n",
        "\n",
        "\n",
        "def read_set_from_file(filename):\n",
        "    '''\n",
        "    Extract a de-duped collection (set) of text from a file.\n",
        "    Expected file format is one item per line.\n",
        "    '''\n",
        "    collection = set()\n",
        "    with open(filename, 'r', encoding='utf-8') as file_:\n",
        "        for line in file_:\n",
        "            collection.add(line.rstrip())\n",
        "    return collection\n",
        "\n",
        "\n",
        "def get_file_extension(path, dot=True, lower=True):\n",
        "    ext = os.path.splitext(path)[1]\n",
        "    ext = ext if dot else ext[1:]\n",
        "    return ext.lower() if lower else ext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Aukz6DTzGM",
        "outputId": "07c03322-28f1-4026-ef32-44328e0e785f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-30 10:47:48--  https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 54.161.5.137, 34.225.34.242, 34.224.55.150, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.161.5.137|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc [following]\n",
            "--2022-04-30 10:47:48--  https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.84.18.124, 52.84.18.102, 52.84.18.77, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.84.18.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607066087 (579M) [application/x-gzip]\n",
            "Saving to: ‘MARBERT_pytorch_verison.tar.gz’\n",
            "\n",
            "MARBERT_pytorch_ver 100%[===================>] 578.94M  93.0MB/s    in 6.0s    \n",
            "\n",
            "2022-04-30 10:47:54 (96.0 MB/s) - ‘MARBERT_pytorch_verison.tar.gz’ saved [607066087/607066087]\n",
            "\n",
            "MARBERT_pytorch_verison/\n",
            "MARBERT_pytorch_verison/pytorch_model.bin\n",
            "MARBERT_pytorch_verison/config.json\n",
            "MARBERT_pytorch_verison/vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz\n",
        "\n",
        "!tar -xvf MARBERT_pytorch_verison.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vanYjKxCLoeb"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin\",\n",
        "    'bert-base-german-cased': \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-pytorch_model.bin\",\n",
        "    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin\",\n",
        "    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin\",\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
        "    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin\",\n",
        "    'marbert':'MARBERT_pytorch_verison/pytorch_model.bin',\n",
        "}\n",
        "PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json\",\n",
        "    'bert-base-german-cased': \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-config.json\",\n",
        "    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json\",\n",
        "    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json\",\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json\",\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json\",\n",
        "    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json\",\n",
        "    'marbert':'MARBERT_pytorch_verison/config.json',\n",
        "}\n",
        "BERT_CONFIG_NAME = 'bert_config.json'\n",
        "TF_WEIGHTS_NAME = 'model.ckpt'\n",
        "\n",
        "def prune_linear_layer(layer, index, dim=0):\n",
        "    \"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    index = index.to(layer.weight.device)\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\n",
        "    if layer.bias is not None:\n",
        "        if dim == 1:\n",
        "            b = layer.bias.clone().detach()\n",
        "        else:\n",
        "            b = layer.bias[index].clone().detach()\n",
        "    new_size = list(layer.weight.size())\n",
        "    new_size[dim] = len(index)\n",
        "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n",
        "    new_layer.weight.requires_grad = False\n",
        "    new_layer.weight.copy_(W.contiguous())\n",
        "    new_layer.weight.requires_grad = True\n",
        "    if layer.bias is not None:\n",
        "        new_layer.bias.requires_grad = False\n",
        "        new_layer.bias.copy_(b.contiguous())\n",
        "        new_layer.bias.requires_grad = True\n",
        "    return new_layer\n",
        "\n",
        "\n",
        "def load_tf_weights_in_bert(model, tf_checkpoint_path):\n",
        "    \"\"\" Load tf checkpoints in a pytorch model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import re\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        print(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\")\n",
        "        raise\n",
        "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
        "    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "    for name, shape in init_vars:\n",
        "        print(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array)\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name.split('/')\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n",
        "            print(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        pointer = model\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n",
        "                l = re.split(r'_(\\d+)', m_name)\n",
        "            else:\n",
        "                l = [m_name]\n",
        "            if l[0] == 'kernel' or l[0] == 'gamma':\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif l[0] == 'output_bias' or l[0] == 'beta':\n",
        "                pointer = getattr(pointer, 'bias')\n",
        "            elif l[0] == 'output_weights':\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif l[0] == 'squad':\n",
        "                pointer = getattr(pointer, 'classifier')\n",
        "            else:\n",
        "                try:\n",
        "                    pointer = getattr(pointer, l[0])\n",
        "                except AttributeError:\n",
        "                    print(\"Skipping {}\".format(\"/\".join(name)))\n",
        "                    continue\n",
        "            if len(l) >= 2:\n",
        "                num = int(l[1])\n",
        "                pointer = pointer[num]\n",
        "        if m_name[-11:] == '_embeddings':\n",
        "            pointer = getattr(pointer, 'weight')\n",
        "        elif m_name == 'kernel':\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            assert pointer.shape == array.shape\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        print(\"Initialize PyTorch weight {}\".format(name))\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "    return model\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02,\n",
        "                 layer_norm_eps=1e-12):\n",
        "        \"\"\"Constructs BertConfig.\n",
        "\n",
        "        Args:\n",
        "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "            layer_norm_eps: The epsilon used by LayerNorm.\n",
        "        \"\"\"\n",
        "        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n",
        "                        and isinstance(vocab_size_or_config_json_file, unicode)):\n",
        "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.num_attention_heads = num_attention_heads\n",
        "            self.hidden_act = hidden_act\n",
        "            self.intermediate_size = intermediate_size\n",
        "            self.hidden_dropout_prob = hidden_dropout_prob\n",
        "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "            self.layer_norm_eps = layer_norm_eps\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \"or the path to a pretrained model config file (str)\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "    def to_json_file(self, json_file_path):\n",
        "        \"\"\" Save this instance to a json file.\"\"\"\n",
        "        with open(json_file_path, \"w\", encoding='utf-8') as writer:\n",
        "            writer.write(self.to_json_string())\n",
        "\n",
        "try:\n",
        "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
        "except ImportError:\n",
        "    logger.info(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\")\n",
        "    class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.output_attentions = output_attentions\n",
        "        self.keep_multihead_output = keep_multihead_output\n",
        "        self.multihead_output = None\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        if self.keep_multihead_output:\n",
        "            self.multihead_output = context_layer\n",
        "            self.multihead_output.retain_grad()\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        if self.output_attentions:\n",
        "            return attention_probs, context_layer\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.output_attentions = output_attentions\n",
        "        self.self = BertSelfAttention(config, output_attentions=output_attentions,\n",
        "                                              keep_multihead_output=keep_multihead_output)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n",
        "        for head in heads:\n",
        "            mask[head] = 0\n",
        "        mask = mask.view(-1).contiguous().eq(1)\n",
        "        index = torch.arange(len(mask))[mask].long()\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "        # Update hyper params\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask, head_mask=None):\n",
        "        self_output = self.self(input_tensor, attention_mask, head_mask)\n",
        "        if self.output_attentions:\n",
        "            attentions, self_output = self_output\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        if self.output_attentions:\n",
        "            return attentions, attention_output\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.output_attentions = output_attentions\n",
        "        self.attention = BertAttention(config, output_attentions=output_attentions,\n",
        "                                               keep_multihead_output=keep_multihead_output)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        attention_output = self.attention(hidden_states, attention_mask, head_mask)\n",
        "        if self.output_attentions:\n",
        "            attentions, attention_output = attention_output\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        if self.output_attentions:\n",
        "            return attentions, layer_output\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        self.output_attentions = output_attentions\n",
        "        layer = BertLayer(config, output_attentions=output_attentions,\n",
        "                                  keep_multihead_output=keep_multihead_output)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, head_mask=None):\n",
        "        all_encoder_layers = []\n",
        "        all_attentions = []\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            hidden_states = layer_module(hidden_states, attention_mask, head_mask[i])\n",
        "            if self.output_attentions:\n",
        "                attentions, hidden_states = hidden_states\n",
        "                all_attentions.append(attentions)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        if self.output_attentions:\n",
        "            return all_attentions, all_encoder_layers\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPredictionHeadTransform, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertLMPredictionHead, self).__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n",
        "                                 bert_model_embedding_weights.size(0),\n",
        "                                 bias=False)\n",
        "        self.decoder.weight = bert_model_embedding_weights\n",
        "        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertOnlyMLMHead, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyNSPHead, self).__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainedModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(BertPreTrainedModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "\n",
        "        Params:\n",
        "            pretrained_model_name_or_path: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-large-cased`\n",
        "                    . `bert-base-multilingual-uncased`\n",
        "                    . `bert-base-multilingual-cased`\n",
        "                    . `bert-base-chinese`\n",
        "                    . `bert-base-german-cased`\n",
        "                    . `bert-large-uncased-whole-word-masking`\n",
        "                    . `bert-large-cased-whole-word-masking`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `model.chkpt` a TensorFlow checkpoint\n",
        "            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n",
        "            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n",
        "            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        state_dict = kwargs.get('state_dict', None)\n",
        "        kwargs.pop('state_dict', None)\n",
        "        cache_dir = kwargs.get('cache_dir', None)\n",
        "        kwargs.pop('cache_dir', None)\n",
        "        from_tf = kwargs.get('from_tf', False)\n",
        "        kwargs.pop('from_tf', None)\n",
        "\n",
        "        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
        "            config_file = PRETRAINED_CONFIG_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
        "        else:\n",
        "            if from_tf:\n",
        "                # Directly load from a TensorFlow checkpoint\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME)\n",
        "                config_file = os.path.join(pretrained_model_name_or_path, BERT_CONFIG_NAME)\n",
        "            else:\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
        "                config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "                logger.error(\n",
        "                    \"Couldn't reach server at '{}' to download pretrained weights.\".format(\n",
        "                        archive_file))\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                    \"associated to this path or url.\".format(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                        archive_file))\n",
        "            return None\n",
        "        try:\n",
        "            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            if pretrained_model_name_or_path in PRETRAINED_CONFIG_ARCHIVE_MAP:\n",
        "                logger.error(\n",
        "                    \"Couldn't reach server at '{}' to download pretrained model configuration file.\".format(\n",
        "                        config_file))\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                    \"associated to this path or url.\".format(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        ', '.join(PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
        "                        config_file))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file and resolved_config_file == config_file:\n",
        "            logger.info(\"loading weights file {}\".format(archive_file))\n",
        "            logger.info(\"loading configuration file {}\".format(config_file))\n",
        "        else:\n",
        "            logger.info(\"loading weights file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "            logger.info(\"loading configuration file {} from cache at {}\".format(\n",
        "                config_file, resolved_config_file))\n",
        "        # Load config\n",
        "        config = BertConfig.from_json_file(resolved_config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        if state_dict is None and not from_tf:\n",
        "            state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
        "        if from_tf:\n",
        "            # Directly load from a TensorFlow checkpoint\n",
        "            return load_tf_weights_in_bert(model, weights_path)\n",
        "        # Load from a PyTorch state_dict\n",
        "        old_keys = []\n",
        "        new_keys = []\n",
        "        for key in state_dict.keys():\n",
        "            new_key = None\n",
        "            if 'gamma' in key:\n",
        "                new_key = key.replace('gamma', 'weight')\n",
        "            if 'beta' in key:\n",
        "                new_key = key.replace('beta', 'bias')\n",
        "            if new_key:\n",
        "                old_keys.append(key)\n",
        "                new_keys.append(new_key)\n",
        "        for old_key, new_key in zip(old_keys, new_keys):\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        start_prefix = ''\n",
        "        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n",
        "            start_prefix = 'bert.'\n",
        "        load(model, prefix=start_prefix)\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if len(error_msgs) > 0:\n",
        "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
        "                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
        "        return model\n",
        "\n",
        "\n",
        "class BertModel(BertPreTrainedModel):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "\n",
        "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "            classifier pretrained on top of the hidden state associated to the first character of the\n",
        "            input (`CLS`) to train on the Next-Sentence task (see BERT's paper).\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config, output_attentions=output_attentions,\n",
        "                                           keep_multihead_output=keep_multihead_output)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def get_multihead_outputs(self):\n",
        "        \"\"\" Gather all multi-head outputs.\n",
        "            Return: list (layers) of multihead module outputs with gradients\n",
        "        \"\"\"\n",
        "        return [layer.attention.self.multihead_output for layer in self.encoder.layer]\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, head_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        if head_mask is not None:\n",
        "            if head_mask.dim() == 1:\n",
        "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "                head_mask = head_mask.expand_as(self.config.num_hidden_layers, -1, -1, -1, -1)\n",
        "            elif head_mask.dim() == 2:\n",
        "                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
        "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
        "        else:\n",
        "            head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers,\n",
        "                                      head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, encoded_layers = encoded_layers\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        if self.output_attentions:\n",
        "            return all_attentions, encoded_layers, pooled_output\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "class BertForPreTraining(BertPreTrainedModel):\n",
        "    \"\"\"BERT model with pre-training heads.\n",
        "    This module comprises the BERT model followed by the two pre-training heads:\n",
        "        - the masked language modeling head, and\n",
        "        - the next sentence classification head.\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
        "            is only computed for the labels set in [0, ..., vocab_size]\n",
        "        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, 1].\n",
        "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "    Outputs:\n",
        "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
        "            sentence classification loss.\n",
        "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
        "            Outputs a tuple comprising\n",
        "            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n",
        "            - the next sentence classification logits of shape [batch_size, 2].\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    model = BertForPreTraining(config)\n",
        "    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertForPreTraining, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.bert = BertModel(config, output_attentions=output_attentions,\n",
        "                                      keep_multihead_output=keep_multihead_output)\n",
        "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                                   output_all_encoded_layers=False, head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, sequence_output, pooled_output = outputs\n",
        "        else:\n",
        "            sequence_output, pooled_output = outputs\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        if masked_lm_labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "            return total_loss\n",
        "        elif self.output_attentions:\n",
        "            return all_attentions, prediction_scores, seq_relationship_score\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertForMaskedLM(BertPreTrainedModel):\n",
        "    \"\"\"BERT model with the masked language modeling head.\n",
        "    This module comprises the BERT model followed by the masked language modeling head.\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
        "            is only computed for the labels set in [0, ..., vocab_size]\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "    Outputs:\n",
        "        if `masked_lm_labels` is  not `None`:\n",
        "            Outputs the masked language modeling loss.\n",
        "        if `masked_lm_labels` is `None`:\n",
        "            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    model = BertForMaskedLM(config)\n",
        "    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertForMaskedLM, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.bert = BertModel(config, output_attentions=output_attentions,\n",
        "                                      keep_multihead_output=keep_multihead_output)\n",
        "        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                       output_all_encoded_layers=False,\n",
        "                                       head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, sequence_output, _ = outputs\n",
        "        else:\n",
        "            sequence_output, _ = outputs\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        if masked_lm_labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
        "            return masked_lm_loss\n",
        "        elif self.output_attentions:\n",
        "            return all_attentions, prediction_scores\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
        "    \"\"\"BERT model with next sentence prediction head.\n",
        "    This module comprises the BERT model followed by the next sentence classification head.\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, 1].\n",
        "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "    Outputs:\n",
        "        if `next_sentence_label` is not `None`:\n",
        "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
        "            sentence classification loss.\n",
        "        if `next_sentence_label` is `None`:\n",
        "            Outputs the next sentence classification logits of shape [batch_size, 2].\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    model = BertForNextSentencePrediction(config)\n",
        "    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertForNextSentencePrediction, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.bert = BertModel(config, output_attentions=output_attentions,\n",
        "                                      keep_multihead_output=keep_multihead_output)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                     output_all_encoded_layers=False,\n",
        "                                     head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, _, pooled_output = outputs\n",
        "        else:\n",
        "            _, pooled_output = outputs\n",
        "        seq_relationship_score = self.cls(pooled_output)\n",
        "\n",
        "        if next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            return next_sentence_loss\n",
        "        elif self.output_attentions:\n",
        "            return all_attentions, seq_relationship_score\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    num_labels = 2\n",
        "\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=2, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config, output_attentions=output_attentions,\n",
        "                                      keep_multihead_output=keep_multihead_output)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, _, pooled_output = outputs\n",
        "        else:\n",
        "            _, pooled_output = outputs\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            return loss\n",
        "        elif self.output_attentions:\n",
        "            return all_attentions, logits\n",
        "        return logits\n",
        "\n",
        "\n",
        "class BertForMultipleChoice(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for multiple choice tasks.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "        `num_choices`: the number of classes for the classifier. Default = 2.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n",
        "            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n",
        "            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_choices].\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n",
        "    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n",
        "    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    num_choices = 2\n",
        "\n",
        "    model = BertForMultipleChoice(config, num_choices)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_choices=2, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertForMultipleChoice, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.num_choices = num_choices\n",
        "        self.bert = BertModel(config, output_attentions=output_attentions,\n",
        "                                      keep_multihead_output=keep_multihead_output)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, head_mask=None):\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        outputs = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False, head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, _, pooled_output = outputs\n",
        "        else:\n",
        "            _, pooled_output = outputs\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, self.num_choices)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "            return loss\n",
        "        elif self.output_attentions:\n",
        "            return all_attentions, reshaped_logits\n",
        "        return reshaped_logits\n",
        "\n",
        "\n",
        "class BertForTokenClassification(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for token-level classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the full hidden state of the last layer.\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    num_labels = 2\n",
        "\n",
        "    model = BertForTokenClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels=2, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertForTokenClassification, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel(config, output_attentions=output_attentions,\n",
        "                                      keep_multihead_output=keep_multihead_output)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, sequence_output, _ = outputs\n",
        "        else:\n",
        "            sequence_output, _ = outputs\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            return loss\n",
        "        elif self.output_attentions:\n",
        "            return all_attentions, logits\n",
        "        return logits\n",
        "\n",
        "\n",
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model\n",
        "        `output_attentions`: If True, also output attentions weights computed by the model at each layer. Default: False\n",
        "        `keep_multihead_output`: If True, saves output of the multi-head attention module with its gradient.\n",
        "            This can be used to compute head importance metrics. Default: False\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "        `head_mask`: an optional torch.Tensor of shape [num_heads] or [num_layers, num_heads] with indices between 0 and 1.\n",
        "            It's a mask to be used to nullify some heads of the transformer. 1.0 => head is fully masked, 0.0 => head is not masked.\n",
        "\n",
        "    Outputs:\n",
        "        if `start_positions` and `end_positions` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
        "        if `start_positions` or `end_positions` is `None`:\n",
        "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
        "            position tokens of shape [batch_size, sequence_length].\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.output_attentions = output_attentions\n",
        "        self.bert = BertModel(config, output_attentions=output_attentions,\n",
        "                                      keep_multihead_output=keep_multihead_output)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,\n",
        "                end_positions=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                                       output_all_encoded_layers=False,\n",
        "                                                       head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, sequence_output, _ = outputs\n",
        "        else:\n",
        "            sequence_output, _ = outputs\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        elif self.output_attentions:\n",
        "            return all_attentions, start_logits, end_logits\n",
        "        return start_logits, end_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEKx1xiWIueL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import math\n",
        "\n",
        "\n",
        "class VocabGraphConvolution(nn.Module):\n",
        "   \n",
        "    def __init__(self,voc_dim, num_adj, hid_dim, out_dim, dropout_rate=0.2):\n",
        "        super(VocabGraphConvolution, self).__init__()\n",
        "        self.voc_dim=voc_dim\n",
        "        self.num_adj=num_adj\n",
        "        self.hid_dim=hid_dim\n",
        "        self.out_dim=out_dim\n",
        "\n",
        "        for i in range(self.num_adj):\n",
        "            setattr(self, 'W%d_vh'%i, nn.Parameter(torch.randn(voc_dim, hid_dim)))\n",
        "\n",
        "        self.fc_hc=nn.Linear(hid_dim,out_dim) \n",
        "        self.act_func = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for n,p in self.named_parameters():\n",
        "            if n.startswith('W') or n.startswith('a') or n in ('W','a','dense'):\n",
        "                init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "\n",
        "    def forward(self, vocab_adj_list, X_dv, add_linear_mapping_term=False):\n",
        "        for i in range(self.num_adj):\n",
        "            H_vh=vocab_adj_list[i].mm(getattr(self, 'W%d_vh'%i))\n",
        "            # H_vh=self.dropout(F.elu(H_vh))\n",
        "            H_vh=self.dropout(H_vh)\n",
        "            H_dh=X_dv.matmul(H_vh)\n",
        "\n",
        "            if add_linear_mapping_term:\n",
        "                H_linear=X_dv.matmul(getattr(self, 'W%d_vh'%i))\n",
        "                H_linear=self.dropout(H_linear)\n",
        "                H_dh+=H_linear\n",
        "\n",
        "            if i == 0:\n",
        "                fused_H = H_dh\n",
        "            else:\n",
        "                fused_H += H_dh\n",
        "\n",
        "        out=self.fc_hc(fused_H)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Pretrain_VGCN(nn.Module):\n",
        "    \n",
        "    def __init__(self, word_emb, word_emb_dim, gcn_adj_dim, gcn_adj_num, gcn_embedding_dim, num_labels,dropout_rate=0.2):\n",
        "        super(Pretrain_VGCN, self).__init__()\n",
        "        self.act_func = nn.LeakyReLU()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.word_emb=word_emb\n",
        "        self.vocab_gcn=VocabGraphConvolution(gcn_adj_dim, gcn_adj_num, 128, gcn_embedding_dim) #192/256\n",
        "        self.classifier = nn.Linear(gcn_embedding_dim*word_emb_dim, num_labels)\n",
        "    def forward(self, vocab_adj_list, gcn_swop_eye, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        words_embeddings = self.word_emb(input_ids)\n",
        "        vocab_input=gcn_swop_eye.matmul(words_embeddings).transpose(1,2)\n",
        "        gcn_vocab_out = self.vocab_gcn(vocab_adj_list, vocab_input).transpose(1,2)\n",
        "        gcn_vocab_out=self.dropout(self.act_func(gcn_vocab_out))\n",
        "        out=self.classifier(gcn_vocab_out.flatten(start_dim=1))\n",
        "        return out\n",
        "\n",
        "\n",
        "class VGCNBertEmbeddings(BertEmbeddings):\n",
        "   \n",
        "    def __init__(self, config, gcn_adj_dim, gcn_adj_num, gcn_embedding_dim):\n",
        "        super(VGCNBertEmbeddings, self).__init__(config)\n",
        "        assert gcn_embedding_dim>=0\n",
        "        self.gcn_embedding_dim=gcn_embedding_dim\n",
        "        self.vocab_gcn=VocabGraphConvolution(gcn_adj_dim, gcn_adj_num, 128, gcn_embedding_dim) #192/256\n",
        "\n",
        "    def forward(self, vocab_adj_list, gcn_swop_eye, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        vocab_input=gcn_swop_eye.matmul(words_embeddings).transpose(1,2)\n",
        "        \n",
        "        if self.gcn_embedding_dim>0:\n",
        "            gcn_vocab_out = self.vocab_gcn(vocab_adj_list, vocab_input)\n",
        "         \n",
        "            gcn_words_embeddings=words_embeddings.clone()\n",
        "            for i in range(self.gcn_embedding_dim):\n",
        "                tmp_pos=(attention_mask.sum(-1)-2-self.gcn_embedding_dim+1+i)+torch.arange(0,input_ids.shape[0]).to(input_ids.device)*input_ids.shape[1]\n",
        "                gcn_words_embeddings.flatten(start_dim=0, end_dim=1)[tmp_pos,:]=gcn_vocab_out[:,:,i]\n",
        "\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        if self.gcn_embedding_dim>0:\n",
        "            embeddings = gcn_words_embeddings + position_embeddings + token_type_embeddings\n",
        "        else:\n",
        "            embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class VGCN_Bert(BertModel):\n",
        "    def __init__(self, config, gcn_adj_dim, gcn_adj_num, gcn_embedding_dim, num_labels, output_attentions=False, keep_multihead_output=False):\n",
        "        super(VGCN_Bert, self).__init__(config,output_attentions,keep_multihead_output)\n",
        "        self.embeddings = VGCNBertEmbeddings(config,gcn_adj_dim,gcn_adj_num, gcn_embedding_dim)\n",
        "        self.encoder = BertEncoder(config, output_attentions=output_attentions,\n",
        "                                           keep_multihead_output=keep_multihead_output)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.num_labels=num_labels\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.will_collect_cls_states=False\n",
        "        self.all_cls_states=[]\n",
        "        self.output_attentions=output_attentions\n",
        "\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, vocab_adj_list, gcn_swop_eye, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, head_mask=None):\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        embedding_output = self.embeddings(vocab_adj_list, gcn_swop_eye, input_ids, token_type_ids,attention_mask)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask. \n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        if head_mask is not None:\n",
        "            if head_mask.dim() == 1:\n",
        "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "                head_mask = head_mask.expand_as(self.config.num_hidden_layers, -1, -1, -1, -1)\n",
        "            elif head_mask.dim() == 2:\n",
        "                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
        "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
        "        else:\n",
        "            head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        if self.output_attentions:\n",
        "            output_all_encoded_layers=True\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers,\n",
        "                                      head_mask=head_mask)\n",
        "        if self.output_attentions:\n",
        "            all_attentions, encoded_layers = encoded_layers\n",
        "\n",
        "        pooled_output = self.pooler(encoded_layers[-1])\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        if self.output_attentions:\n",
        "            return all_attentions, logits\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfjefMLdWg57",
        "outputId": "b78b2517-5ec3-4e52-d1d7-2ddc34672970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.26.0,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.25.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.4->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.4->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.4->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch_pretrained_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "Wc9C800p6OJN",
        "outputId": "fde0234b-a754-453f-a7bb-392f2c4e80dd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6c21181b7660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvDIIeiAKQW9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import argparse\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "import random\n",
        "def train():\n",
        "    random.seed(44)\n",
        "    np.random.seed(44)\n",
        "    torch.manual_seed(44)\n",
        "\n",
        "    cuda_yes = torch.cuda.is_available()\n",
        "    if cuda_yes:\n",
        "        torch.cuda.manual_seed_all(44)\n",
        "    device = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")\n",
        "\n",
        "    \n",
        "    ds='ASTD'\n",
        "    load= 0\n",
        "    sw=True\n",
        "    dim=32\n",
        "    lr= 4e-06\n",
        "    l2= 2e-04\n",
        "    model='VGCN_BERT'\n",
        "    cfg_ds = ds\n",
        "    cfg_model_type = model\n",
        "    cfg_stop_words = True if sw == 1 else False\n",
        "    will_train_mode_from_checkpoint = True if load == 1 else False\n",
        "    gcn_embedding_dim = dim\n",
        "    learning_rate0 = lr\n",
        "    l2_decay = l2\n",
        "    model_file_save='model.pt'\n",
        "\n",
        "\n",
        "    dataset_list = {'sst', 'cola', 'AJGT', 'ASTD', 'ArSarcasm', 'ArSAS'}\n",
        "    # hate: 10k, mr: 6753, sst: 7792, r8: 5211\n",
        "\n",
        "    total_train_epochs = 20\n",
        "    dropout_rate = 0.2  # 0.5 # Dropout rate (1 - keep probability).\n",
        "    if cfg_ds == 'sst':\n",
        "        batch_size = 16  # 12\n",
        "        learning_rate0 = 1e-5  # 2e-5\n",
        "        # l2_decay = 0.001\n",
        "        l2_decay = 0.01  # default\n",
        "    elif cfg_ds == 'cola':\n",
        "        batch_size = 16  # 12\n",
        "        learning_rate0 = 8e-6  # 2e-5\n",
        "        l2_decay = 0.001\n",
        "    elif cfg_ds == 'AJGT':\n",
        "        batch_size = 16  # 12\n",
        "        learning_rate0 = 2e-06  # 2e-5\n",
        "        l2_decay = 0.01\n",
        "    elif cfg_ds == 'ASTD':\n",
        "        batch_size = 12  # 12\n",
        "        learning_rate0 = 4e-06  # 2e-5\n",
        "        l2_decay = 2e-04\n",
        "    elif cfg_ds == 'ArSarcasm':\n",
        "        batch_size = 12  # 12\n",
        "        learning_rate0 = 2e-5  # 2e-5\n",
        "        l2_decay = 2e-04\n",
        "    elif cfg_ds == 'ArSAS':\n",
        "        batch_size = 32  # 12\n",
        "        learning_rate0 = 2e-5  # 2e-5\n",
        "        l2_decay = 0.01\n",
        "    MAX_SEQ_LENGTH = 200 + gcn_embedding_dim\n",
        "    gradient_accumulation_steps = 1\n",
        "    bert_model_scale = 'marbert'\n",
        "    do_lower_case = True\n",
        "    warmup_proportion = 0.1\n",
        "\n",
        "    data_dir = 'drive/MyDrive/Data/dump_data'\n",
        "    output_dir = 'output/'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "\n",
        "    perform_metrics_str = ['weighted avg', 'f1-score']\n",
        "\n",
        "    # cfg_add_linear_mapping_term=False\n",
        "    cfg_vocab_adj = 'pmi'\n",
        "    # cfg_vocab_adj='all'\n",
        "    # cfg_vocab_adj='tf'\n",
        "    cfg_adj_npmi_threshold = 0.2\n",
        "    cfg_adj_tf_threshold = 0\n",
        "    classifier_act_func = nn.ReLU()\n",
        "\n",
        "    resample_train_set = False  # if mse and resample, then do resample\n",
        "    do_softmax_before_mse = True\n",
        "    cfg_loss_criterion = 'cle'\n",
        "    \n",
        "    print(cfg_model_type + ' Start at:', time.asctime())\n",
        "    print('\\n----- Configure -----',\n",
        "          '\\n  cfg_ds:', cfg_ds,\n",
        "          '\\n  stop_words:', cfg_stop_words,\n",
        "          # '\\n  Vocab GCN_hidden_dim: 768 -> 1152 -> 768',\n",
        "          '\\n  Vocab GCN_hidden_dim: vocab_size -> 128 -> ' + str(gcn_embedding_dim),\n",
        "          '\\n  Learning_rate0:', learning_rate0, 'weight_decay:', l2_decay,\n",
        "          '\\n  Loss_criterion', cfg_loss_criterion, 'softmax_before_mse', do_softmax_before_mse,\n",
        "          '\\n  Dropout:', dropout_rate, 'Run_adj:', cfg_vocab_adj, 'gcn_act_func: Relu',\n",
        "          '\\n  MAX_SEQ_LENGTH:', MAX_SEQ_LENGTH,  # 'valid_data_taux',valid_data_taux,\n",
        "          '\\n  perform_metrics_str:', perform_metrics_str)\n",
        "\n",
        "    # %%\n",
        "    '''\n",
        "    Prepare data set\n",
        "    Load vocabulary adjacent matrix\n",
        "    '''\n",
        "    print('\\n----- Prepare data set -----')\n",
        "    print('  Load/shuffle/seperate', cfg_ds, 'dataset, and vocabulary graph adjacent matrix')\n",
        "\n",
        "    objects = []\n",
        "    names = ['labels', 'train_y', 'train_y_prob', 'valid_y', 'valid_y_prob', 'test_y', 'test_y_prob',\n",
        "             'shuffled_clean_docs', 'vocab_adj_tf', 'vocab_adj_pmi', 'vocab_map']\n",
        "    for i in range(len(names)):\n",
        "        datafile = \"../\" + data_dir + \"/data_%s.%s\" % (cfg_ds, names[i])\n",
        "        with open(datafile, 'rb') as f:\n",
        "            objects.append(pkl.load(f, encoding='latin1'))\n",
        "    lables_list, train_y, train_y_prob, valid_y, valid_y_prob, test_y, test_y_prob, shuffled_clean_docs, gcn_vocab_adj_tf, gcn_vocab_adj, gcn_vocab_map = tuple(\n",
        "        objects)\n",
        "    # lables_list=[0,1]\n",
        "    label2idx = lables_list[0]\n",
        "    idx2label = lables_list[1]\n",
        "\n",
        "    y = np.hstack((train_y, valid_y, test_y))\n",
        "    y_prob = np.vstack((train_y_prob, valid_y_prob, test_y_prob))\n",
        "\n",
        "    examples = []\n",
        "    for i, ts in enumerate(shuffled_clean_docs):\n",
        "        ex = InputExample(i, ts.strip(), confidence=y_prob[i], label=y[i])\n",
        "        examples.append(ex)\n",
        "\n",
        "    num_classes = len(label2idx)\n",
        "    gcn_vocab_size = len(gcn_vocab_map)\n",
        "    train_size = len(train_y)\n",
        "    valid_size = len(valid_y)\n",
        "    test_size = len(test_y)\n",
        "\n",
        "    indexs = np.arange(0, len(examples))\n",
        "    train_examples = [examples[i] for i in indexs[:train_size]]\n",
        "    valid_examples = [examples[i] for i in indexs[train_size:train_size + valid_size]]\n",
        "    test_examples = [examples[i] for i in indexs[train_size + valid_size:train_size + valid_size + test_size]]\n",
        "\n",
        "    if cfg_adj_tf_threshold > 0:\n",
        "        gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > cfg_adj_tf_threshold)\n",
        "        gcn_vocab_adj_tf.eliminate_zeros()\n",
        "    if cfg_adj_npmi_threshold > 0:\n",
        "        gcn_vocab_adj.data *= (gcn_vocab_adj.data > cfg_adj_npmi_threshold)\n",
        "        gcn_vocab_adj.eliminate_zeros()\n",
        "\n",
        "    if cfg_vocab_adj == 'pmi':\n",
        "        gcn_vocab_adj_list = [gcn_vocab_adj]\n",
        "    elif cfg_vocab_adj == 'tf':\n",
        "        gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
        "    elif cfg_vocab_adj == 'all':\n",
        "        gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj]\n",
        "\n",
        "    norm_gcn_vocab_adj_list = []\n",
        "    for i in range(len(gcn_vocab_adj_list)):\n",
        "        adj = gcn_vocab_adj_list[i]  # .tocsr() #(lr是用非norm时的1/10)\n",
        "        print('  Zero ratio(?>66%%) for vocab adj %dth: %.8f' % (\n",
        "        i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
        "        adj = normalize_adj(adj)\n",
        "        norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
        "    gcn_adj_list = norm_gcn_vocab_adj_list\n",
        "\n",
        "    del gcn_vocab_adj_tf, gcn_vocab_adj, gcn_vocab_adj_list\n",
        "    gc.collect()\n",
        "\n",
        "    train_classes_num, train_classes_weight = get_class_count_and_weight(train_y, len(label2idx))\n",
        "    loss_weight = torch.tensor(train_classes_weight).to(device)\n",
        "    model_path='MARBERT_pytorch_verison'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)                 \n",
        "\n",
        "    #tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n",
        "\n",
        "    # %%\n",
        "\n",
        "    def get_pytorch_dataloader(examples, tokenizer, batch_size, shuffle_choice, classes_weight=None,\n",
        "                               total_resample_size=-1):\n",
        "        ds = CorpusDataset(examples, tokenizer, gcn_vocab_map, MAX_SEQ_LENGTH, gcn_embedding_dim)\n",
        "        if shuffle_choice == 0:  # shuffle==False\n",
        "            return DataLoader(dataset=ds,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False,\n",
        "                              num_workers=0,\n",
        "                              collate_fn=ds.pad)\n",
        "        elif shuffle_choice == 1:  # shuffle==True\n",
        "            return DataLoader(dataset=ds,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=0,\n",
        "                              collate_fn=ds.pad)\n",
        "        elif shuffle_choice == 2:  # weighted resampled\n",
        "            assert classes_weight is not None\n",
        "            assert total_resample_size > 0\n",
        "            weights = [classes_weight[0] if label == 0 else classes_weight[1] if label == 1 else classes_weight[2] for\n",
        "                       _, _, _, _, label in dataset]\n",
        "            sampler = WeightedRandomSampler(weights, num_samples=total_resample_size, replacement=True)\n",
        "            return DataLoader(dataset=ds,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=sampler,\n",
        "                              num_workers=0,\n",
        "                              collate_fn=ds.pad)\n",
        "\n",
        "    train_dataloader = get_pytorch_dataloader(train_examples, tokenizer, batch_size, shuffle_choice=0)\n",
        "    valid_dataloader = get_pytorch_dataloader(valid_examples, tokenizer, batch_size, shuffle_choice=0)\n",
        "    test_dataloader = get_pytorch_dataloader(test_examples, tokenizer, batch_size, shuffle_choice=0)\n",
        "\n",
        "    # total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
        "    total_train_steps = int(len(train_dataloader) / gradient_accumulation_steps * total_train_epochs)\n",
        "\n",
        "    print('  Train_classes count:', train_classes_num)\n",
        "    print('  Num examples for train =', len(train_examples), ', after weight sample:',\n",
        "          len(train_dataloader) * batch_size)\n",
        "    print(\"  Num examples for validate = %d\" % len(valid_examples))\n",
        "    print(\"  Batch size = %d\" % batch_size)\n",
        "    print(\"  Num steps = %d\" % total_train_steps)\n",
        "\n",
        "    # %%\n",
        "    '''\n",
        "    Train vgcn_bert model\n",
        "    '''\n",
        "\n",
        "    def predict(model, examples, tokenizer, batch_size):\n",
        "        dataloader = get_pytorch_dataloader(examples, tokenizer, batch_size, shuffle_choice=0)\n",
        "        predict_out = []\n",
        "        confidence_out = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(dataloader):\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, _, label_ids, gcn_swop_eye = batch\n",
        "                score_out = model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
        "                if cfg_loss_criterion == 'mse' and do_softmax_before_mse:\n",
        "                    score_out = torch.nn.functional.softmax(score_out, dim=-1)\n",
        "                predict_out.extend(score_out.max(1)[1].tolist())\n",
        "                confidence_out.extend(score_out.max(1)[0].tolist())\n",
        "\n",
        "        return np.array(predict_out).reshape(-1), np.array(confidence_out).reshape(-1)\n",
        "\n",
        "    def evaluate(model, gcn_adj_list, predict_dataloader, batch_size, epoch_th, dataset_name):\n",
        "        # print(\"***** Running prediction *****\")\n",
        "        model.eval()\n",
        "        predict_out = []\n",
        "        all_label_ids = []\n",
        "        ev_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            for batch in predict_dataloader:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
        "                # the parameter label_ids is None, model return the prediction score\n",
        "                logits = model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
        "\n",
        "                if cfg_loss_criterion == 'mse':\n",
        "                    if do_softmax_before_mse:\n",
        "                        logits = F.softmax(logits, -1)\n",
        "                    loss = F.mse_loss(logits, y_prob)\n",
        "                else:\n",
        "                    if loss_weight is None:\n",
        "                        loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
        "                    else:\n",
        "                        loss = F.cross_entropy(logits.view(-1, num_classes), label_ids)\n",
        "                ev_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(logits, -1)\n",
        "                predict_out.extend(predicted.tolist())\n",
        "                all_label_ids.extend(label_ids.tolist())\n",
        "                eval_accuracy = predicted.eq(label_ids).sum().item()\n",
        "                total += len(label_ids)\n",
        "                correct += eval_accuracy\n",
        "\n",
        "            f1_metrics = f1_score(np.array(all_label_ids).reshape(-1),\n",
        "                                  np.array(predict_out).reshape(-1), average='weighted')\n",
        "            print(\"Report:\\n\" + classification_report(np.array(all_label_ids).reshape(-1),\n",
        "                                                      np.array(predict_out).reshape(-1), digits=4))\n",
        "\n",
        "        ev_acc = correct / total\n",
        "        end = time.time()\n",
        "        print('Epoch : %d, %s: %.3f Acc : %.3f on %s, Spend:%.3f minutes for evaluation'\n",
        "              % (epoch_th, ' '.join(perform_metrics_str), 100 * f1_metrics, 100. * ev_acc, dataset_name,\n",
        "                 (end - start) / 60.0))\n",
        "        print('--------------------------------------------------------------')\n",
        "        return ev_loss, ev_acc, f1_metrics\n",
        "    # %%\n",
        "    print(\"\\n----- Running training -----\")\n",
        "    if will_train_mode_from_checkpoint and os.path.exists(os.path.join(output_dir, model_file_save)):\n",
        "        checkpoint = torch.load(os.path.join(output_dir, model_file_save), map_location='cpu')\n",
        "        if 'step' in checkpoint:\n",
        "            prev_save_step = checkpoint['step']\n",
        "            start_epoch = checkpoint['epoch']\n",
        "        else:\n",
        "            prev_save_step = -1\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "        valid_acc_prev = checkpoint['valid_acc']\n",
        "        perform_metrics_prev = checkpoint['perform_metrics']\n",
        "        model_path='MARBERT_pytorch_verison'\n",
        "        ##model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(label2idx))\n",
        "        model = VGCN_Bert.from_pretrained(bert_model_scale, state_dict=checkpoint['model_state'],\n",
        "                                          gcn_adj_dim=gcn_vocab_size, gcn_adj_num=len(gcn_adj_list),\n",
        "                                          gcn_embedding_dim=gcn_embedding_dim, num_labels=len(label2idx))\n",
        "        pretrained_dict = checkpoint['model_state']\n",
        "        net_state_dict = model.state_dict()\n",
        "        pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n",
        "        net_state_dict.update(pretrained_dict_selected)\n",
        "        model.load_state_dict(net_state_dict)\n",
        "        print('Loaded the pretrain model:', model_file_save, ', epoch:', checkpoint['epoch'], 'step:', prev_save_step,\n",
        "              'valid acc:',\n",
        "              checkpoint['valid_acc'], ' '.join(perform_metrics_str) + '_valid:', checkpoint['perform_metrics'])\n",
        "\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        valid_acc_prev = 0\n",
        "        perform_metrics_prev = 0\n",
        "        model = VGCN_Bert.from_pretrained(bert_model_scale, gcn_adj_dim=gcn_vocab_size, gcn_adj_num=len(gcn_adj_list),\n",
        "                                          gcn_embedding_dim=gcn_embedding_dim, num_labels=len(label2idx))\n",
        "        prev_save_step = -1\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = BertAdam(model.parameters(), lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps,\n",
        "                         weight_decay=l2_decay)\n",
        "\n",
        "    train_start = time.time()\n",
        "    global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n",
        "\n",
        "    all_loss_list = {'train': [], 'valid': [], 'test': []}\n",
        "    all_f1_list = {'train': [], 'valid': [], 'test': []}\n",
        "    for epoch in range(start_epoch, total_train_epochs):\n",
        "        tr_loss = 0\n",
        "        ep_train_start = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        #for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if prev_save_step > -1:\n",
        "                if step <= prev_save_step: continue\n",
        "            if prev_save_step > -1:\n",
        "                prev_save_step = -1\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, input_mask, segment_ids, y_prob, label_ids, gcn_swop_eye = batch\n",
        "\n",
        "            logits = model(gcn_adj_list, gcn_swop_eye, input_ids, segment_ids, input_mask)\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if cfg_loss_criterion == 'mse':\n",
        "                if do_softmax_before_mse:\n",
        "                    logits = F.softmax(logits, -1)\n",
        "                loss = F.mse_loss(logits, y_prob)\n",
        "            else:\n",
        "                if loss_weight is None:\n",
        "                    loss = F.cross_entropy(logits, label_ids)\n",
        "                else:\n",
        "                    loss = F.cross_entropy(logits.view(-1, num_classes), label_ids, loss_weight.float())\n",
        "\n",
        "            if gradient_accumulation_steps > 1:\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step_th += 1\n",
        "            if step % 40 == 0:\n",
        "                print(\n",
        "                    \"Epoch:{}-{}/{}, Train {} Loss: {}, Cumulated time: {}m \".format(epoch, step, len(train_dataloader),\n",
        "                                                                                     cfg_loss_criterion, loss.item(), (\n",
        "                                                                                                 time.time() - train_start) / 60.0))\n",
        "\n",
        "        print('--------------------------------------------------------------')\n",
        "        valid_loss, valid_acc, perform_metrics = evaluate(model, gcn_adj_list, valid_dataloader, batch_size, epoch,\n",
        "                                                          'Valid_set')\n",
        "        test_loss, _, test_f1 = evaluate(model, gcn_adj_list, test_dataloader, batch_size, epoch, 'Test_set')\n",
        "        all_loss_list['train'].append(tr_loss)\n",
        "        all_loss_list['valid'].append(valid_loss)\n",
        "        all_loss_list['test'].append(test_loss)\n",
        "        all_f1_list['valid'].append(perform_metrics)\n",
        "        all_f1_list['test'].append(test_f1)\n",
        "        print(\"Epoch:{} completed, Total Train Loss:{}, Valid Loss:{}, Spend {}m \".format(epoch, tr_loss, valid_loss, (\n",
        "                    time.time() - train_start) / 60.0))\n",
        "        # Save a checkpoint\n",
        "        # if valid_acc > valid_acc_prev:\n",
        "        if perform_metrics > perform_metrics_prev:\n",
        "            to_save = {'epoch': epoch, 'model_state': model.state_dict(),\n",
        "                       'valid_acc': valid_acc, 'lower_case': do_lower_case,\n",
        "                       'perform_metrics': perform_metrics}\n",
        "            #torch.save(to_save, os.path.join(output_dir, model_file_save))\n",
        "            # valid_acc_prev = valid_acc\n",
        "            perform_metrics_prev = perform_metrics\n",
        "            test_f1_when_valid_best = test_f1\n",
        "            # train_f1_when_valid_best=tr_f1\n",
        "            valid_f1_best_epoch = epoch\n",
        "\n",
        "    print('\\n**Optimization Finished!,Total spend:', (time.time() - train_start) / 60.0)\n",
        "    print(\"**Valid weighted F1: %.3f at %d epoch.\" % (100 * perform_metrics_prev, valid_f1_best_epoch))\n",
        "    print(\"**Test weighted F1 when valid best: %.3f\" % (100 * test_f1_when_valid_best))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acUUPh1mY5JA"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}